{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {
        "id": "cell-0"
      },
      "source": [
        "# Exercise 3: Mathematical Problem Solving with LLMs\n",
        "\n",
        "**This is a marked exercise (graded)**\n",
        "\n",
        "Apply LLMs to solve mathematical reasoning tasks. Test different pre-trained models with various prompting strategies and optionally fine-tune with LoRA to improve performance.\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Evaluate LLMs on mathematical reasoning\n",
        "- Design effective prompts for numerical tasks\n",
        "- Implement and compare different prompting strategies\n",
        "- Optionally: Fine-tune models using LoRA\n",
        "- Measure performance using accuracy metric with tolerance\n",
        "\n",
        "**Deliverables:**\n",
        "- Completed notebook with your approach\n",
        "- `submission.csv` with predictions on test set (100 problems)\n",
        "- Score: Accuracy with 2 decimal precision tolerance (threshold: 70%)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {
        "id": "cell-1"
      },
      "source": [
        "## Part 1: Setup and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cell-2",
      "metadata": {
        "id": "cell-2"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch peft datasets pandas scikit-learn matplotlib requests -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cell-3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-3",
        "outputId": "233fd483-fbc5-4c3e-f88e-264e057bc435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import re\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {
        "id": "cell-4"
      },
      "source": [
        "## Part 2: Download Dataset\n",
        "\n",
        "Download the math problem dataset (1000 problems: 900 train, 100 test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cell-5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-5",
        "outputId": "a1fb28b1-b593-41a4-d64e-13812c22a013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded train.csv\n",
            "Downloaded test.csv\n"
          ]
        }
      ],
      "source": [
        "# URLs for the dataset files\n",
        "base_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module8/exercise/'\n",
        "\n",
        "train_url = base_url + 'train.csv'\n",
        "test_url = base_url + 'test.csv'\n",
        "\n",
        "def download_file(url, filename):\n",
        "    \"\"\"Download a file from URL.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded {filename}\")\n",
        "\n",
        "# Download files\n",
        "download_file(train_url, 'train.csv')\n",
        "download_file(test_url, 'test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cell-6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-6",
        "outputId": "aae22fd7-3b05-490d-a723-891383ecbe00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 900\n",
            "Test set size: 100\n",
            "\n",
            "Training set category distribution:\n",
            "category\n",
            "algebra          150\n",
            "arithmetic       153\n",
            "fractions        143\n",
            "geometry         155\n",
            "percentage       152\n",
            "word_problems    147\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample training problems:\n",
            "   id       category                                            problem  \\\n",
            "0   0     percentage                                Increase 109 by 25%   \n",
            "1   1     arithmetic                                   What is 76 + 55?   \n",
            "2   2  word_problems  Sarah has $286. She spends $128. How much mone...   \n",
            "3   3       geometry  What is the circumference of a circle with rad...   \n",
            "4   4       geometry   What is the volume of a cube with side length 3?   \n",
            "5   5     percentage                                 What is 7% of 132?   \n",
            "6   6  word_problems  John is 10 years old now. How old was he 15 ye...   \n",
            "7   7      fractions                  What is 1/5 + 2/5? (decimal form)   \n",
            "8   8     percentage                                What is 27% of 164?   \n",
            "9   9      fractions                  What is 2/4 + 1/4? (decimal form)   \n",
            "\n",
            "   solution  \n",
            "0    136.25  \n",
            "1    131.00  \n",
            "2    158.00  \n",
            "3     87.92  \n",
            "4     27.00  \n",
            "5      9.24  \n",
            "6     -5.00  \n",
            "7      0.60  \n",
            "8     44.28  \n",
            "9      0.75  \n"
          ]
        }
      ],
      "source": [
        "# Load the datasets\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "print(f\"Train set size: {len(train_data)}\")\n",
        "print(f\"Test set size: {len(test_data)}\")\n",
        "\n",
        "# Display category distribution\n",
        "print(\"\\nTraining set category distribution:\")\n",
        "print(train_data['category'].value_counts().sort_index())\n",
        "\n",
        "print(\"\\nSample training problems:\")\n",
        "print(train_data.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {
        "id": "cell-7"
      },
      "source": [
        "## Part 3: Baseline - Dummy Model\n",
        "\n",
        "Create a baseline to understand what poor performance looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cell-8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-8",
        "outputId": "6e0c6f3f-39a6-4c43-a16c-18ce70dc3f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy model (always predicts mean): 150.79\n",
            "This demonstrates very poor performance. Your model should do much better!\n"
          ]
        }
      ],
      "source": [
        "def check_accuracy(predictions, ground_truth, tolerance=0.01):\n",
        "    \"\"\"\n",
        "    Calculate accuracy with tolerance for floating point comparisons.\n",
        "\n",
        "    Two values are considered equal if their difference is <= tolerance\n",
        "    OR if they round to the same value at 2 decimal places.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    for pred, truth in zip(predictions, ground_truth):\n",
        "        # Check if both round to same 2 decimal places\n",
        "        if round(pred, 2) == round(truth, 2):\n",
        "            correct += 1\n",
        "        # Or if absolute difference is very small\n",
        "        elif abs(pred - truth) <= tolerance:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(predictions)\n",
        "\n",
        "# Dummy baseline: always predict the mean\n",
        "mean_solution = train_data['solution'].mean()\n",
        "print(f\"Dummy model (always predicts mean): {mean_solution:.2f}\")\n",
        "print(\"This demonstrates very poor performance. Your model should do much better!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-9",
      "metadata": {
        "id": "cell-9"
      },
      "source": [
        "## Part 4: Utility Functions\n",
        "\n",
        "Helper functions to extract numerical answers from model outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cell-10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-10",
        "outputId": "59386267-abef-4ec1-8cb2-736258a1f4ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number extraction tests:\n",
            "  'The answer is 42' -> 42.0\n",
            "  '42' -> 42.0\n",
            "  '15 + 27 = 42' -> 42.0\n",
            "  'Calculating... the result is 42.5!' -> 42.5\n",
            "  'No number here' -> None\n",
            "  'The value is -15' -> -15.0\n"
          ]
        }
      ],
      "source": [
        "def extract_number(text):\n",
        "    \"\"\"\n",
        "    Extract the first number from text. Return None if no number found.\n",
        "\n",
        "    Handles various formats:\n",
        "    - \"The answer is 42\"\n",
        "    - \"42\"\n",
        "    - \"= 42\"\n",
        "    - \"Result: 42.5\"\n",
        "    - Negative numbers: \"-15\"\n",
        "    \"\"\"\n",
        "    # Try different patterns in order of specificity\n",
        "    patterns = [\n",
        "        r'(?:answer|result|equals?|=)\\s*:?\\s*(-?\\d+\\.?\\d*)',  # \"answer is 42\" or \"= 42\"\n",
        "        r'(-?\\d+\\.?\\d*)\\s*$',  # Number at the end\n",
        "        r'(-?\\d+\\.?\\d*)',  # Any number\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, text, re.IGNORECASE)\n",
        "        if match:\n",
        "            try:\n",
        "                return float(match.group(1))\n",
        "            except (ValueError, IndexError):\n",
        "                continue\n",
        "\n",
        "    return None\n",
        "\n",
        "# Test extraction\n",
        "test_strings = [\n",
        "    \"The answer is 42\",\n",
        "    \"42\",\n",
        "    \"15 + 27 = 42\",\n",
        "    \"Calculating... the result is 42.5!\",\n",
        "    \"No number here\",\n",
        "    \"The value is -15\"\n",
        "]\n",
        "\n",
        "print(\"Number extraction tests:\")\n",
        "for s in test_strings:\n",
        "    result = extract_number(s)\n",
        "    print(f\"  '{s}' -> {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {
        "id": "cell-11"
      },
      "source": [
        "## Part 5: Load Pre-trained Model\n",
        "\n",
        "Load a small, efficient model for math problem solving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {
        "id": "cell-12"
      },
      "outputs": [],
      "source": [
        "# TODO: Load a pre-trained model\n",
        "# Suggested models:\n",
        "# - \"gpt2\" (small, fast)\n",
        "# - \"microsoft/phi-2\" (better reasoning, needs more memory)\n",
        "# - \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" (good balance)\n",
        "\n",
        "# model_name = \"gpt2\"  # Start with GPT-2\n",
        "model_name = \"LiquidAI/LFM2-350M-Math\"\n",
        "model_name = \"LiquidAI/LFM2-350M-Math-GGUF\"\n",
        "\n",
        "print(f\"Loading model: {model_name}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model = model.to(device)\n",
        "\n",
        "# Set padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-13",
      "metadata": {
        "id": "cell-13"
      },
      "source": [
        "## Part 6: Prompting Strategies\n",
        "\n",
        "Test different prompt templates to improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cell-14",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-14",
        "outputId": "314165a4-75f0-442c-93e8-5b2706a1b250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing problem: Increase 109 by 25%\n",
            "Correct answer: 136.25\n",
            "\n",
            "======================================================================\n",
            "✗ simple:\n",
            "  Response: 109\n",
            "\n",
            "Answer: 109\n",
            "\n",
            "Answer: 109\n",
            "\n",
            "Answer: 109\n",
            "\n",
            "Answer: 109\n",
            "\n",
            "Answer: 109\n",
            "\n",
            "Answer: 109\n",
            "\n",
            "Answer: 109\n",
            "\n",
            "Answ...\n",
            "  Extracted: 109.0\n",
            "\n",
            "✗ instruction:\n",
            "  Response: Increase 109 by 25%\n",
            "Ease: Increase 109 by 25%\n",
            "Ease: Increase 109 by 25%\n",
            "Ease: Increase 109 by 25%\n",
            "Ea...\n",
            "  Extracted: 109.0\n",
            "\n",
            "✗ cot:\n",
            "  Response: First, let's let's let's let's let's let's let's let's let's let's let's let's let's let's let's let...\n",
            "  Extracted: None\n",
            "\n",
            "✗ few_shot:\n",
            "  Response: 158.0\n",
            "\n",
            "Problem: Increase 15 by 25%\n",
            "Answer: 175.0\n",
            "\n",
            "Problem: Increase 16 by 25%\n",
            "Answer: 175.0\n",
            "\n",
            "Problem...\n",
            "  Extracted: 175.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def generate_answer(problem, prompt_template=\"simple\", max_new_tokens=50, temperature=0.1):\n",
        "    \"\"\"\n",
        "    Generate answer using different prompt templates.\n",
        "\n",
        "    Templates:\n",
        "    - simple: Just the problem\n",
        "    - instruction: Add instruction to solve\n",
        "    - cot: Chain-of-thought prompting\n",
        "    - few_shot: Include examples from training data\n",
        "    \"\"\"\n",
        "    if prompt_template == \"simple\":\n",
        "        prompt = f\"{problem}\\nAnswer:\"\n",
        "\n",
        "    elif prompt_template == \"instruction\":\n",
        "        prompt = f\"Solve this math problem and provide only the numerical answer.\\n\\nProblem: {problem}\\nAnswer:\"\n",
        "\n",
        "    elif prompt_template == \"cot\":\n",
        "        prompt = f\"Solve this math problem step by step, then provide the final numerical answer.\\n\\nProblem: {problem}\\nSolution:\\n\"\n",
        "\n",
        "    elif prompt_template == \"few_shot\":\n",
        "        # Include 3-5 examples from training data\n",
        "        examples = []\n",
        "        for i in range(min(5, len(train_data))):\n",
        "            examples.append(f\"Problem: {train_data['problem'].iloc[i]}\\nAnswer: {train_data['solution'].iloc[i]}\")\n",
        "\n",
        "        examples_text = \"\\n\\n\".join(examples)\n",
        "        prompt = f\"{examples_text}\\n\\nProblem: {problem}\\nAnswer:\"\n",
        "\n",
        "    else:\n",
        "        prompt = problem\n",
        "\n",
        "    # Generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=True if temperature > 0 else False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove the prompt from response\n",
        "    response = response[len(prompt):].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Test different prompts on a sample problem\n",
        "test_problem = train_data['problem'].iloc[0]\n",
        "test_solution = train_data['solution'].iloc[0]\n",
        "\n",
        "print(f\"Testing problem: {test_problem}\")\n",
        "print(f\"Correct answer: {test_solution}\\n\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for template in [\"simple\", \"instruction\", \"cot\", \"few_shot\"]:\n",
        "    response = generate_answer(test_problem, template)\n",
        "    extracted = extract_number(response)\n",
        "\n",
        "    correct = \"✓\" if extracted is not None and round(extracted, 2) == round(test_solution, 2) else \"✗\"\n",
        "\n",
        "    print(f\"{correct} {template}:\")\n",
        "    print(f\"  Response: {response[:100]}{'...' if len(response) > 100 else ''}\")\n",
        "    print(f\"  Extracted: {extracted}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-15",
      "metadata": {
        "id": "cell-15"
      },
      "source": [
        "## Part 7: Evaluate on Validation Set\n",
        "\n",
        "Test your best prompting strategy on a subset of training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-16",
        "outputId": "63cb322d-0b15-46ed-b49e-3261f1d2a868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on 50 validation problems...\n",
            "\n",
            "Processed 10/50 problems...\n",
            "Processed 20/50 problems...\n"
          ]
        }
      ],
      "source": [
        "# TODO: Choose your best prompt template\n",
        "best_template = \"few_shot\"  # Change based on your experiments\n",
        "\n",
        "# Evaluate on a small validation set (last 50 training examples)\n",
        "val_data = train_data.tail(50)\n",
        "\n",
        "predictions = []\n",
        "ground_truth = val_data['solution'].tolist()\n",
        "\n",
        "print(f\"Evaluating on {len(val_data)} validation problems...\\n\")\n",
        "\n",
        "for idx, row in val_data.iterrows():\n",
        "    problem = row['problem']\n",
        "    solution = row['solution']\n",
        "\n",
        "    response = generate_answer(problem, prompt_template=best_template)\n",
        "    prediction = extract_number(response)\n",
        "\n",
        "    # If no number extracted, use 0 (will be wrong)\n",
        "    if prediction is None:\n",
        "        prediction = 0.0\n",
        "\n",
        "    predictions.append(prediction)\n",
        "\n",
        "    if (len(predictions) % 10) == 0:\n",
        "        print(f\"Processed {len(predictions)}/{len(val_data)} problems...\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = check_accuracy(predictions, ground_truth)\n",
        "print(f\"\\nValidation Accuracy: {accuracy:.2%}\")\n",
        "print(f\"Need to achieve: 70% on test set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {
        "id": "cell-17"
      },
      "source": [
        "## Part 8: Generate Test Predictions\n",
        "\n",
        "Generate predictions for the test set and create submission file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-18",
      "metadata": {
        "id": "cell-18"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate predictions on test set\n",
        "print(f\"Generating predictions on {len(test_data)} test problems...\\n\")\n",
        "\n",
        "test_predictions = []\n",
        "\n",
        "for idx, row in test_data.iterrows():\n",
        "    problem = row['problem']\n",
        "\n",
        "    response = generate_answer(problem, prompt_template=best_template)\n",
        "    prediction = extract_number(response)\n",
        "\n",
        "    # If no number extracted, use 0\n",
        "    if prediction is None:\n",
        "        prediction = 0.0\n",
        "        print(f\"⚠️  Warning: No number extracted for problem {idx}: {problem[:50]}...\")\n",
        "\n",
        "    test_predictions.append(prediction)\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "        print(f\"Processed {idx + 1}/{len(test_data)} problems...\")\n",
        "\n",
        "print(\"\\nAll test predictions generated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-19",
      "metadata": {
        "id": "cell-19"
      },
      "source": [
        "## Part 9: Create Submission File\n",
        "\n",
        "Save predictions in the required format for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-20",
      "metadata": {
        "id": "cell-20"
      },
      "outputs": [],
      "source": [
        "# Create submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_data['id'],\n",
        "    'solution': test_predictions\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Submission file created: submission.csv\")\n",
        "print(\"\\nSubmission preview:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "# Verify all predictions are numerical\n",
        "non_numeric = submission['solution'].isna().sum()\n",
        "if non_numeric > 0:\n",
        "    print(f\"\\n⚠️  WARNING: {non_numeric} predictions are not numerical!\")\n",
        "    print(\"These will result in incorrect answers. Please fix them.\")\n",
        "else:\n",
        "    print(\"\\n✓ All predictions are numerical\")\n",
        "\n",
        "# Show statistics\n",
        "print(\"\\nPrediction statistics:\")\n",
        "print(submission['solution'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-21",
      "metadata": {
        "id": "cell-21"
      },
      "source": [
        "## Part 10 (Optional): Fine-Tuning with LoRA\n",
        "\n",
        "If prompting doesn't achieve 70% accuracy, consider fine-tuning with LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-22",
      "metadata": {
        "id": "cell-22"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement LoRA fine-tuning (OPTIONAL)\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# This is a template - implement if needed\n",
        "print(\"LoRA fine-tuning is optional.\")\n",
        "print(\"Use this if prompting strategies don't achieve 70% accuracy.\")\n",
        "print(\"\\nConsider:\")\n",
        "print(\"- Prepare training dataset in correct format\")\n",
        "print(\"- Configure LoRA parameters (r=8, alpha=32)\")\n",
        "print(\"- Train for a few epochs\")\n",
        "print(\"- Evaluate and compare with prompting approaches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-23",
      "metadata": {
        "id": "cell-23"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions:\n",
        "\n",
        "1. **Which prompting strategy worked best and why?**\n",
        "   - YOUR ANSWER HERE\n",
        "\n",
        "2. **What types of math problems were most challenging for the model?**\n",
        "   - YOUR ANSWER HERE\n",
        "\n",
        "3. **How did you handle number extraction from model outputs?**\n",
        "   - YOUR ANSWER HERE\n",
        "\n",
        "4. **What are the limitations of using LLMs for mathematical reasoning?**\n",
        "   - YOUR ANSWER HERE\n",
        "\n",
        "5. **If you used LoRA fine-tuning, what were the trade-offs compared to prompting?**\n",
        "   - YOUR ANSWER HERE (or N/A if you didn't use LoRA)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}